# Import necessary libraries
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, round

# Function to filter trips with a distance of 2.0 miles or longer
def long_trips(trips):
    return trips.filter(col("trip_distance") >= 2.0)

# Function to determine the top 20 drop-off locations in Manhattan
def manhattan_trips(trips, lookup):
    manhattan_locations = lookup.filter(col("Borough") == "Manhattan") \
                                .groupBy("LocationID") \
                                .sum("passenger_count") \
                                .withColumnRenamed("sum(passenger_count)", "pcount") \
                                .orderBy(col("pcount").desc()) \
                                .limit(20)
    return manhattan_locations

# Function to calculate the weighted profit value for each pick-up location
def weighted_profit(trips, mtrips):
    total_trips = trips.count()
    total_mtrips = mtrips.selectExpr("sum(pcount) as total_passengers").collect()[0]["total_passengers"]
    weighted_profit = trips.groupBy("PULocationID") \
                          .agg({"total_amount": "avg", "passenger_count": "count"}) \
                          .withColumnRenamed("avg(total_amount)", "avg_total_amount") \
                          .withColumnRenamed("count(passenger_count)", "total_trips") \
                          .join(mtrips, trips.PULocationID == mtrips.DOLocationID, "left_outer") \
                          .withColumn("proportion", col("pcount") / total_mtrips) \
                          .withColumn("weighted_profit", col("avg_total_amount") * col("proportion")) \
                          .select("PULocationID", "weighted_profit")
    return weighted_profit

# Function to provide Borough and Zone information for the top 20 pick-up locations
def final_output(wp, lookup):
    final_output = wp.join(lookup, wp.PULocationID == lookup.LocationID) \
                     .select("Zone", "Borough", "weighted_profit") \
                     .orderBy(col("weighted_profit").desc()) \
                     .limit(20)
    return final_output

# Main function to run the entire process
def main():
    # Initialize Spark session
    spark = SparkSession.builder \
                        .appName("NYC Taxi Analysis") \
                        .getOrCreate()

    # Load trip data and lookup data
    trips = spark.read.csv("s3://your-bucket-name/trip_data.csv", header=True, inferSchema=True)
    lookup = spark.read.csv("s3://your-bucket-name/lookup_data.csv", header=True, inferSchema=True)

    # Perform data analysis
    long_trips_data = long_trips(trips)
    mtrips_data = manhattan_trips(trips, lookup)
    weighted_profit_data = weighted_profit(trips, mtrips_data)
    final_output_data = final_output(weighted_profit_data, lookup)

    # Write the final output to CSV
    final_output_data.write.csv("s3://your-bucket-name/output.csv", header=True)

    # Stop Spark session
    spark.stop()

# Run the main function
if __name__ == "__main__":
    main()
