from pyspark.ml.feature import CountVectorizer
from pyspark.ml.clustering import LDA

# Create a Bag of Words (BoW) representation of the text data
count_vectorizer = CountVectorizer(inputCol="cleaned", outputCol="features")
count_vectorizer_model = count_vectorizer.fit(processed_df)
bag_of_words = count_vectorizer_model.transform(processed_df)

# Train LDA model
num_topics = 5
lda = LDA(k=num_topics, seed=123, optimizer="em")
lda_model = lda.fit(bag_of_words)

# Get the topics and associated words
topics = lda_model.describeTopics(maxTermsPerTopic=5)
