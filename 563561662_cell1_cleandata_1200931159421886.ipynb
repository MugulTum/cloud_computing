from pyspark.ml import Pipeline
from sparknlp.annotator import *
from sparknlp.base import *

# Load the dataset
df = spark.read.csv("<INPUT_PATH>", header=True, inferSchema=True)

# Initialize Spark NLP annotators
document_assembler = DocumentAssembler().setInputCol("text").setOutputCol("document")
tokenizer = Tokenizer().setInputCols(["document"]).setOutputCol("tokenized")
normalizer = Normalizer().setInputCols(["tokenized"]).setOutputCol("normalized")
lemmatizer = LemmatizerModel.pretrained().setInputCols(["normalized"]).setOutputCol("lemmatized")
stopwords_cleaner = StopWordsCleaner().setInputCols(["lemmatized"]).setOutputCol("cleaned")

# Create Spark NLP pipeline
pipeline = Pipeline(stages=[
    document_assembler,
    tokenizer,
    normalizer,
    lemmatizer,
    stopwords_cleaner
])

# Fit the pipeline to the data
pipeline_model = pipeline.fit(df)
processed_df = pipeline_model.transform(df)
