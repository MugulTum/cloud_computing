from pyspark.sql.functions import explode, col

# Explode the cleaned tokens into separate rows
exploded_df = processed_df.select(explode("cleaned").alias("word"))

# Count the frequency of each word
word_counts = exploded_df.groupBy("word").count().orderBy(col("count").desc())

# Get the top 20 most frequent words
top_words = word_counts.limit(20)
